{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 5)\t1\n",
      "[[1 1 1 1 1 0 1]\n",
      " [1 1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print X\n",
    "print X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to']\n",
      "[[1 1 1 1 1 0 1]\n",
      " [1 1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "print vectorizer.get_feature_names()\n",
    "print X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01.txt', '02.txt', '03.txt', '04.txt', '05.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print os.listdir('toy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n"
     ]
    }
   ],
   "source": [
    "f = open('toy/01.txt', 'r')\n",
    "print f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a toy post about machine learning. Actually, it contains not much interesting stuff.', 'Imaging databases provide storage capabilities.', 'Most imaging databases safe images permanently.', 'Imaging databases store data.', 'Imaging databases store data. Imaging databases store data. Imaging databases store data.']\n"
     ]
    }
   ],
   "source": [
    "DIR = 'toy'\n",
    "posts = [] \n",
    "for f in os.listdir('toy'):\n",
    "    #print os.path.join(DIR, f)\n",
    "    posts.append(open(os.path.join(DIR, f)).read())\n",
    "print posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = [open(os.path.join(DIR, f)).read() for f in os.listdir('toy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'this', u'toy']\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 25)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'this', u'toy']\n",
      "[[1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1]\n",
      " [0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.get_feature_names()\n",
    "print X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a,b = 1,2\n",
    "print a\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "a,b=1,2\n",
    "c = a\n",
    "a = b\n",
    "b = c\n",
    "print a,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "a,b = b,a\n",
    "print a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 25)\n",
      "5\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "#num_samples, num_features = X_train.shape\n",
    "num_samples, num_features = X_train.shape\n",
    "print num_samples\n",
    "print num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n",
      "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'this', u'toy']\n"
     ]
    }
   ],
   "source": [
    "print(\"#samples: %d, #features: %d\" % (num_samples,num_features)) \n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25)\n"
     ]
    }
   ],
   "source": [
    "print new_post_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=2.00: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=1.41: Imaging databases store data.\n",
      "=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=1.41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_raw(post_vec, new_post_vec)\n",
    "    print \"=== Post %i with dist=%.2f: %s\"%(i, d, post)\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76536686473\n"
     ]
    }
   ],
   "source": [
    "print dist_norm(X_train[4], new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.92: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print \"=== Post %i with dist=%.2f: %s\"%(i, d, post)\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "print len(vectorizer.get_stop_words())\n",
    "sorted(vectorizer.get_stop_words())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'learning', u'machine', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'toy']\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print \"=== Post %i with dist=%.2f: %s\"%(i, d, post)\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphic\n",
      "imag\n",
      "imag\n",
      "imagin\n",
      "imagin\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "s= nltk.stem.SnowballStemmer('english')\n",
    "print s.stem(\"graphics\")\n",
    "print s.stem(\"imaging\")\n",
    "print s.stem(\"image\")\n",
    "print s.stem(\"imagination\")\n",
    "print s.stem(\"imagine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'safe', u'storag', u'store', u'stuff', u'toy']\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "db = sqlite3.connect('news2.sqlite') \n",
    "cur = db.cursor()\n",
    "cur.execute('select title, summary, category from news_entry')\n",
    "allNews = cur.fetchall()\n",
    "\n",
    "corpus = []\n",
    "tags = []\n",
    "title = []\n",
    "for rec in allNews:\n",
    "    #if (rec[2] == '娛樂'.decode('utf-8')) or \\\n",
    "    #   (rec[2] == '社會'.decode('utf-8')) or \\\n",
    "    #   (rec[2] == '財經'.decode('utf-8'))  :\n",
    "    corpus.append(' '.join(jieba.cut(rec[1])))\n",
    "    tags.append(rec[2]) \n",
    "cur.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "print tags.count('財經'.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n"
     ]
    }
   ],
   "source": [
    "print len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word = vectorizer.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples, features =  X.shape\n",
    "X2= X.toarray().transpose()\n",
    "#print X2\n",
    "#print X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_data, test_data, train_tag, test_tag = train_test_split(X, tags, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 35298)\n",
      "(451, 35298)\n"
     ]
    }
   ],
   "source": [
    "print train_data.shape\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print X[0].toarray().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy\n",
    "\n",
    "clf = MultinomialNB(alpha=0.01)\n",
    "clf.fit(train_data,train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print test_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.60088691796\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(test_tag, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  2  0  0  2  0  0  0  4  0  1  1  1  1  0  0]\n",
      " [ 1  8  0  0  0  0  0  0  1  0  0  1  0  0  0  0]\n",
      " [ 0  1 31  0  0  6  1  1  9  0  3  4  2  0  1  0]\n",
      " [ 0  0  0  6  0  0  0  0  1  0  0  1  0  0  0  0]\n",
      " [ 1  1  0  0  7  0  0  1 17  0  0  1  1  0  0  0]\n",
      " [ 0  0  0  0  2  2  0  0  2  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 34  0  5  0  1  1  2  1  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  1  0  0  1  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 3  2  1  4  1  2  4  1 22  0 38  5  4  3  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  8  0  0  0  0]\n",
      " [ 0  0  0  0  0  3  3  0 12  0  1  1 53  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  3  0  1  1  0  9  1  1]\n",
      " [ 0  0  1  0  0  1  0  0  1  0  0  5  0  0 32  1]\n",
      " [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0 24]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "a = confusion_matrix(test_tag, pred) \n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rom pandas_confusion import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delimiter = \"，|。|、|（|）|／|《|》|】|【|「|」|；|：|！\".decode('utf-8') \n",
    "def splitSentense(text, delimiter):\n",
    "    return re.split(delimiter, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = allNews[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From rpm-list-admin@freshrpms.net  Sat Oct  5 10:36:14 2002\\n', 'Return-Path: <rpm-zzzlist-admin@freshrpms.net>\\n', 'Delivered-To: yyyy@localhost.spamassassin.taint.org\\n', 'Received: from localhost (jalapeno [127.0.0.1])\\n', '\\tby jmason.org (Postfix) with ESMTP id 6C4D016F21\\n', '\\tfor <jm@localhost>; Sat,  5 Oct 2002 10:35:36 +0100 (IST)\\n', 'Received: from jalapeno [127.0.0.1]\\n', '\\tby localhost with IMAP (fetchmail-5.9.0)\\n', '\\tfor jm@localhost (single-drop); Sat, 05 Oct 2002 10:35:36 +0100 (IST)\\n', 'Received: from egwn.net (auth02.nl.egwn.net [193.172.5.4]) by\\n', '    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g94NEAK13745 for\\n', '    <jm-rpm@jmason.org>; Sat, 5 Oct 2002 00:14:15 +0100\\n', 'Received: from auth02.nl.egwn.net (localhost [127.0.0.1]) by egwn.net\\n', '    (8.11.6/8.11.6/EGWN) with ESMTP id g94N82f30483; Sat, 5 Oct 2002 01:08:02\\n', '    +0200\\n', 'Received: from relay2.EECS.Berkeley.EDU (relay2.EECS.Berkeley.EDU\\n', '    [169.229.60.28]) by egwn.net (8.11.6/8.11.6/EGWN) with ESMTP id\\n', '    g94N7If23138 for <rpm-list@freshrpms.net>; Sat, 5 Oct 2002 01:07:18 +0200\\n', 'Received: from relay3.EECS.Berkeley.EDU (localhost.Berkeley.EDU\\n', '    [127.0.0.1]) by relay2.EECS.Berkeley.EDU (8.9.3/8.9.3) with ESMTP id\\n', '    QAA03058 for <rpm-list@freshrpms.net>; Fri, 4 Oct 2002 16:07:16 -0700\\n', '    (PDT)\\n', 'Received: from eecs.berkeley.edu (brawnix.CS.Berkeley.EDU [128.32.35.162])\\n', '    by relay3.EECS.Berkeley.EDU (8.9.3/8.9.3) with ESMTP id QAA16606 for\\n', '    <rpm-list@freshrpms.net>; Fri, 4 Oct 2002 16:07:11 -0700 (PDT)\\n', 'Message-Id: <3D9E1F20.3050300@eecs.berkeley.edu>\\n', 'From: Ben Liblit <liblit@eecs.berkeley.edu>\\n', 'User-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.1) Gecko/20020827\\n', 'X-Accept-Language: en-us, en\\n', 'MIME-Version: 1.0\\n', 'To: rpm-zzzlist@freshrpms.net\\n', 'Subject: Re: RedHat 8.0 and his own freetype\\n', 'References: <20021004155451.52f9ecd5.matthias_haase@bennewitz.com>\\n', 'Content-Type: text/plain; charset=us-ascii; format=flowed\\n', 'Content-Transfer-Encoding: 7bit\\n', 'X-Mailscanner: Found to be clean, Found to be clean\\n', 'Sender: rpm-zzzlist-admin@freshrpms.net\\n', 'Errors-To: rpm-zzzlist-admin@freshrpms.net\\n', 'X-Beenthere: rpm-zzzlist@freshrpms.net\\n', 'X-Mailman-Version: 2.0.11\\n', 'Precedence: bulk\\n', 'Reply-To: rpm-zzzlist@freshrpms.net\\n', 'List-Help: <mailto:rpm-zzzlist-request@freshrpms.net?subject=help>\\n', 'List-Post: <mailto:rpm-zzzlist@freshrpms.net>\\n', 'List-Subscribe: <http://lists.freshrpms.net/mailman/listinfo/rpm-zzzlist>,\\n', '    <mailto:rpm-list-request@freshrpms.net?subject=subscribe>\\n', 'List-Id: Freshrpms RPM discussion list <rpm-zzzlist.freshrpms.net>\\n', 'List-Unsubscribe: <http://lists.freshrpms.net/mailman/listinfo/rpm-zzzlist>,\\n', '    <mailto:rpm-list-request@freshrpms.net?subject=unsubscribe>\\n', 'List-Archive: <http://lists.freshrpms.net/pipermail/rpm-zzzlist/>\\n', 'X-Original-Date: Fri, 04 Oct 2002 16:07:12 -0700\\n', 'Date: Fri, 04 Oct 2002 16:07:12 -0700\\n', '\\n', 'Matthias Haase wrote:\\n', '> RH ships the code with the bytecode hinter disabled which makes \\n', '> non-AA fonts really ugly.\\n', '> This reqiures only a small change for include/freetype/config/ftoption.h,\\n', '> it is very well documented.\\n', '\\n', 'Red Hat 8.0 ships with the bytecode hinter enabled; I think 7.3 may have \\n', 'as well.\\n', '\\n', 'The enabling change to \"ftoption.h\" is made by Red Hat\\'s SRPM before \\n', 'building.  Take a look at \"freetype-2.1.1-enable-ft2-bci.patch\" from the \\n', \"SRPM; it's pretty clear that this does exactly what needs to be done.\\n\", '\\n', 'So if your fonts look ugly, lack of bytecode hinting is *not* the cause.\\n', '\\n', '\\n', '_______________________________________________\\n', 'RPM-List mailing list <RPM-List@freshrpms.net>\\n', 'http://lists.freshrpms.net/mailman/listinfo/rpm-list\\n', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "f = open('data/easy_ham/01251.793e5c04967cb90191e805dfa619c55a', 'r')\n",
    "msg = f.readlines()\n",
    "print msg\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthias Haase wrote:\n",
      "> RH ships the code with the bytecode hinter disabled which makes \n",
      "> non-AA fonts really ugly.\n",
      "> This reqiures only a small change for include/freetype/config/ftoption.h,\n",
      "> it is very well documented.\n",
      "\n",
      "Red Hat 8.0 ships with the bytecode hinter enabled; I think 7.3 may have \n",
      "as well.\n",
      "\n",
      "The enabling change to \"ftoption.h\" is made by Red Hat's SRPM before \n",
      "building.  Take a look at \"freetype-2.1.1-enable-ft2-bci.patch\" from the \n",
      "SRPM; it's pretty clear that this does exactly what needs to be done.\n",
      "\n",
      "So if your fonts look ugly, lack of bytecode hinting is *not* the cause.\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "RPM-List mailing list <RPM-List@freshrpms.net>\n",
      "http://lists.freshrpms.net/mailman/listinfo/rpm-list\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_blank_index = msg.index('\\n')\n",
    "#print first_blank_index\n",
    "msg = msg[(first_blank_index + 1): ]\n",
    "print ''.join(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthias Haase wrote:\n",
      "> RH ships the code with the bytecode hinter disabled which makes \n",
      "> non-AA fonts really ugly.\n",
      "> This reqiures only a small change for include/freetype/config/ftoption.h,\n",
      "> it is very well documented.\n",
      "\n",
      "Red Hat 8.0 ships with the bytecode hinter enabled; I think 7.3 may have \n",
      "as well.\n",
      "\n",
      "The enabling change to \"ftoption.h\" is made by Red Hat's SRPM before \n",
      "building.  Take a look at \"freetype-2.1.1-enable-ft2-bci.patch\" from the \n",
      "SRPM; it's pretty clear that this does exactly what needs to be done.\n",
      "\n",
      "So if your fonts look ugly, lack of bytecode hinting is *not* the cause.\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "RPM-List mailing list <RPM-List@freshrpms.net>\n",
      "http://lists.freshrpms.net/mailman/listinfo/rpm-list\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_msg(path):    \n",
    "    f = open(path, 'r')\n",
    "    msg = f.readlines()\n",
    "    f.close()\n",
    "    first_blank_index = msg.index('\\n')\n",
    "    msg = msg[(first_blank_index + 1): ]\n",
    "    return ''.join(msg)\n",
    "\n",
    "print get_msg('data/easy_ham/01251.793e5c04967cb90191e805dfa619c55a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filelist = os.listdir('data/easy_ham')\n",
    "#print filelist\n",
    "\n",
    "filelist = [f for f in filelist if f != 'cmds']\n",
    "#print filelist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_msgdir(path):\n",
    "    filelist = os.listdir(path)\n",
    "    filelist = [f for f in filelist if f != 'cmds']\n",
    "    all_msgs = [get_msg(os.path.join(path, f)) for f in filelist]\n",
    "    return all_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#讀取訓練資料集\n",
    "train_spam_messages    = get_msgdir('data/spam')\n",
    "train_easyham_messages = get_msgdir('data/easy_ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#讀取測試資料集\n",
    "test_spam_messages    = get_msgdir('data/spam_2')\n",
    "test_easyham_messages = get_msgdir('data/easy_ham_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin A posted:\n",
      "Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n",
      " limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n",
      " Mount Athos monastic community, was ideal for the patriotic sculpture. \n",
      " \n",
      " As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n",
      " museum, a restored amphitheatre and car park for admiring crowds are\n",
      "planned\n",
      "---------------------\n",
      "So is this mountain limestone or granite?\n",
      "If it's limestone, it'll weather pretty fast.\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/ \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train_spam_messages[1]\n",
    "print train_easyham_messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_msg_words(msg, stopwords = []):\n",
    "    msg = re.sub('3D', '', msg)\n",
    "    \n",
    "    msg = re.sub('<(.|\\n)*?>', ' ', msg)\n",
    "    msg = re.sub('&\\w+;', ' ', msg)\n",
    "\n",
    "    msg = re.sub('_+', '_', msg)\n",
    "    msg_words = set(wordpunct_tokenize(msg.replace('=\\n', '').lower()))\n",
    "\n",
    "    msg_words = msg_words.difference(stopwords)\n",
    "    msg_words = [w for w in msg_words if re.search('[a-zA-Z]', w) and len(w) > 1]\n",
    "    return msg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['code', 'matthias', 'lack', 'disabled', 'done', 'bytecode', 'config', 'rpm', 'your', 'before', 'mailman', 'ships', 'to', 'only', 'pretty', 'include', 'hat', 'building', 'freetype', 'enable', 'which', 'that', 'very', 'exactly', 'lists', 'made', 'not', 'documented', 'freshrpms', 'with', 'bci', 'look', 'this', 'fonts', 'list', 'patch', 'ugly', 'reqiures', 'small', 'srpm', 'clear', 'wrote', 'aa', 'haase', 'enabled', 'is', 'it', 'hinter', 'as', 'at', 'have', 'take', 'really', 'what', 'mailing', 'does', 'listinfo', 'ftoption', 'net', 'if', 'rh', 'red', 'be', 'http', 'may', 'needs', 'from', 'by', 'change', 'ft2', 'for', 'of', 'hinting', 'well', 'non', 'enabling', 'so', 'the', 'makes', 'think', 'cause']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "msg = get_msg('data/easy_ham/01251.793e5c04967cb90191e805dfa619c55a')\n",
    "print get_msg_words(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', 'll', 've']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "sw.extend(['ll', 've'])\n",
    "print sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aa': True, 'the': True, 'code': True, 'matthias': True, 'enabled': True, 'is': True, 'lack': True, 'it': True, 'disabled': True, 'really': True, 'as': True, 'done': True, 'bytecode': True, 'have': True, 'rpm': True, 'your': True, 'before': True, 'what': True, 'rh': True, 'mailman': True, 'for': True, 'may': True, 'haase': True, 'ships': True, 'to': True, 'only': True, 'does': True, 'listinfo': True, 'take': True, 'which': True, 'so': True, 'net': True, 'include': True, 'hat': True, 'red': True, 'ftoption': True, 'be': True, 'freetype': True, 'enable': True, 'http': True, 'pretty': True, 'that': True, 'very': True, 'exactly': True, 'needs': True, 'lists': True, 'well': True, 'non': True, 'ft2': True, 'if': True, 'makes': True, 'not': True, 'from': True, 'documented': True, 'freshrpms': True, 'with': True, 'by': True, 'change': True, 'look': True, 'building': True, 'hinter': True, 'think': True, 'made': True, 'bci': True, 'this': True, 'config': True, 'clear': True, 'fonts': True, 'list': True, 'patch': True, 'enabling': True, 'ugly': True, 'cause': True, 'reqiures': True, 'of': True, 'small': True, 'srpm': True, 'mailing': True, 'hinting': True, 'wrote': True, 'at': True}\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "msg_words = get_msg_words(msg)\n",
    "#print msg_words\n",
    "\n",
    "for  w in msg_words:\n",
    "    features[w] = True\n",
    "\n",
    "print features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_indicator(msg):\n",
    "    features = {}\n",
    "    msg_words = get_msg_words(msg)\n",
    "    for  w in msg_words:\n",
    "        features[w] = True\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features_from_messages(messages, label, feature_extractor):\n",
    "    features_labels = []\n",
    "    for msg in messages:\n",
    "        features = feature_extractor(msg)\n",
    "        features_labels.append((features, label))\n",
    "    return features_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_spam = features_from_messages(train_spam_messages, 'spam', \n",
    "                                        word_indicator)\n",
    "train_ham = features_from_messages(train_easyham_messages, 'ham', \n",
    "                                       word_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print train_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = train_spam + train_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_spam = features_from_messages(test_spam_messages, 'spam',\n",
    "                                       word_indicator)\n",
    "\n",
    "test_ham = features_from_messages(test_easyham_messages, 'ham',\n",
    "                                     word_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Spam accuracy: 97.21%\n",
      "Test Ham accuracy: 87.92%\n"
     ]
    }
   ],
   "source": [
    "print ('Test Spam accuracy: {0:.2f}%'\n",
    "  .format(100 * nltk.classify.accuracy(classifier, test_spam)))\n",
    "\n",
    "print ('Test Ham accuracy: {0:.2f}%'\n",
    "  .format(100 * nltk.classify.accuracy(classifier, test_ham)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                mailings = True             spam : ham    =    124.8 : 1.0\n",
      "                unwanted = True             spam : ham    =     71.6 : 1.0\n",
      "              requesting = True             spam : ham    =     64.9 : 1.0\n",
      "                mortgage = True             spam : ham    =     64.9 : 1.0\n",
      "            unsubscribed = True             spam : ham    =     62.9 : 1.0\n",
      "                 removal = True             spam : ham    =     56.0 : 1.0\n",
      "               sincerely = True             spam : ham    =     49.2 : 1.0\n",
      "             instruction = True             spam : ham    =     48.3 : 1.0\n",
      "         confidentiality = True             spam : ham    =     48.3 : 1.0\n",
      "                  kindly = True             spam : ham    =     46.9 : 1.0\n",
      "                   opted = True             spam : ham    =     44.9 : 1.0\n",
      "                      pa = True             spam : ham    =     44.9 : 1.0\n",
      "                     sir = True             spam : ham    =     44.9 : 1.0\n",
      "              profitable = True             spam : ham    =     42.1 : 1.0\n",
      "              instructed = True             spam : ham    =     41.6 : 1.0\n",
      "              subscriber = True             spam : ham    =     41.6 : 1.0\n",
      "                      mo = True             spam : ham    =     41.6 : 1.0\n",
      "             subscribers = True             spam : ham    =     41.6 : 1.0\n",
      "                  postal = True             spam : ham    =     41.6 : 1.0\n",
      "              assistance = True             spam : ham    =     40.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
