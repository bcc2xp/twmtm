{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 5)\t1\n",
      "[[1 1 1 1 1 0 1]\n",
      " [1 1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print X\n",
    "print X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to']\n",
      "[[1 1 1 1 1 0 1]\n",
      " [1 1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "print vectorizer.get_feature_names()\n",
    "print X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01.txt', '02.txt', '03.txt', '04.txt', '05.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print os.listdir('toy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n"
     ]
    }
   ],
   "source": [
    "f = open('toy/01.txt', 'r')\n",
    "print f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a toy post about machine learning. Actually, it contains not much interesting stuff.', 'Imaging databases provide storage capabilities.', 'Most imaging databases safe images permanently.', 'Imaging databases store data.', 'Imaging databases store data. Imaging databases store data. Imaging databases store data.']\n"
     ]
    }
   ],
   "source": [
    "DIR = 'toy'\n",
    "posts = [] \n",
    "for f in os.listdir('toy'):\n",
    "    #print os.path.join(DIR, f)\n",
    "    posts.append(open(os.path.join(DIR, f)).read())\n",
    "print posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = [open(os.path.join(DIR, f)).read() for f in os.listdir('toy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'this', u'toy']\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 25)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'this', u'toy']\n",
      "[[1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1]\n",
      " [0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.get_feature_names()\n",
    "print X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a,b = 1,2\n",
    "print a\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "a,b=1,2\n",
    "c = a\n",
    "a = b\n",
    "b = c\n",
    "print a,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "a,b = b,a\n",
    "print a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 25)\n",
      "5\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "#num_samples, num_features = X_train.shape\n",
    "num_samples, num_features = X_train.shape\n",
    "print num_samples\n",
    "print num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n",
      "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'this', u'toy']\n"
     ]
    }
   ],
   "source": [
    "print(\"#samples: %d, #features: %d\" % (num_samples,num_features)) \n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25)\n"
     ]
    }
   ],
   "source": [
    "print new_post_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=2.00: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=1.41: Imaging databases store data.\n",
      "=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=1.41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_raw(post_vec, new_post_vec)\n",
    "    print \"=== Post %i with dist=%.2f: %s\"%(i, d, post)\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76536686473\n"
     ]
    }
   ],
   "source": [
    "print dist_norm(X_train[4], new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.92: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print \"=== Post %i with dist=%.2f: %s\"%(i, d, post)\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "print len(vectorizer.get_stop_words())\n",
    "sorted(vectorizer.get_stop_words())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'learning', u'machine', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'toy']\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print \"=== Post %i with dist=%.2f: %s\"%(i, d, post)\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphic\n",
      "imag\n",
      "imag\n",
      "imagin\n",
      "imagin\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "s= nltk.stem.SnowballStemmer('english')\n",
    "print s.stem(\"graphics\")\n",
    "print s.stem(\"imaging\")\n",
    "print s.stem(\"image\")\n",
    "print s.stem(\"imagination\")\n",
    "print s.stem(\"imagine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'safe', u'storag', u'store', u'stuff', u'toy']\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "db = sqlite3.connect('news2.sqlite') \n",
    "cur = db.cursor()\n",
    "cur.execute('select title, summary, category from news_entry')\n",
    "allNews = cur.fetchall()\n",
    "\n",
    "corpus = []\n",
    "tags = []\n",
    "title = []\n",
    "for rec in allNews:\n",
    "    #if (rec[2] == '娛樂'.decode('utf-8')) or \\\n",
    "    #   (rec[2] == '社會'.decode('utf-8')) or \\\n",
    "    #   (rec[2] == '財經'.decode('utf-8'))  :\n",
    "    corpus.append(' '.join(jieba.cut(rec[1])))\n",
    "    tags.append(rec[2]) \n",
    "cur.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "print tags.count('財經'.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n"
     ]
    }
   ],
   "source": [
    "print len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word = vectorizer.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples, features =  X.shape\n",
    "X2= X.toarray().transpose()\n",
    "#print X2\n",
    "#print X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_data, test_data, train_tag, test_tag = train_test_split(X, tags, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 35298)\n",
      "(451, 35298)\n"
     ]
    }
   ],
   "source": [
    "print train_data.shape\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print X[0].toarray().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy\n",
    "\n",
    "clf = MultinomialNB(alpha=0.01)\n",
    "clf.fit(train_data,train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print test_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.60088691796\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(test_tag, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生活 瘋啥 娛樂 特企 FUN 論壇 體育 正妹 地產 搜奇 財經 動物 國際 政治 社會 時尚\n",
      "[[ 4  2  0  0  2  0  0  0  4  0  1  1  1  1  0  0]\n",
      " [ 1  8  0  0  0  0  0  0  1  0  0  1  0  0  0  0]\n",
      " [ 0  1 31  0  0  6  1  1  9  0  3  4  2  0  1  0]\n",
      " [ 0  0  0  6  0  0  0  0  1  0  0  1  0  0  0  0]\n",
      " [ 1  1  0  0  7  0  0  1 17  0  0  1  1  0  0  0]\n",
      " [ 0  0  0  0  2  2  0  0  2  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 34  0  5  0  1  1  2  1  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  1  0  0  1  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 3  2  1  4  1  2  4  1 22  0 38  5  4  3  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  8  0  0  0  0]\n",
      " [ 0  0  0  0  0  3  3  0 12  0  1  1 53  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  3  0  1  1  0  9  1  1]\n",
      " [ 0  0  1  0  0  1  0  0  1  0  0  5  0  0 32  1]\n",
      " [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0 24]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "a = confusion_matrix(test_tag, pred) \n",
    "for ele in set(test_tag):\n",
    "    print ele, \n",
    "print \n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delimiter = \"，|。|、|（|）|／|《|》|】|【|「|」|；|：|！\".decode('utf-8') \n",
    "def splitSentense(text, delimiter):\n",
    "    return re.split(delimiter, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = allNews[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_rank 神戶。 倉敷都帶來了體現各自藝術品味的工藝品。 這些工藝品不僅展現日本風格。 還富有生活情趣。 對於旅遊購物的遊客來說。 參觀 TDW不僅可以感受日本的多元文化。 更能一網打盡日本各地的特產。 另外。 參與這部分的藝術家還有在國際間非常活躍的前衛藝術家草間彌生。 此次她以富士為主題。 展出9件作品。  相信是不容錯過的重點之一。  更多詳情請參考官方網站。 創意為主題的藝術博覽會 “TOKYO DESIGN WEEK。 http://tokyodesignweek.jp/2015/tokyo/ 。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "sents = list(splitSentense(content,delimiter ))\n",
    "vect = TfidfVectorizer()\n",
    "tfidf = vect.fit_transform(sents)\n",
    "tfidf_graph = tfidf*tfidf.T\n",
    "nx_graph = nx.from_scipy_sparse_matrix(tfidf_graph)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "res = sorted(((scores[i],i) for i,s in enumerate(sents)), reverse=True)\n",
    "summary = [sents[i] for _,i in sorted(res[:15])]\n",
    "print 'text_rank', u'。 '.join(summary).replace('\\r','').replace('\\n','')+u'。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.032840715504398377, 32), (0.032840715504398377, 31), (0.032840715504398377, 30), (0.032840715504398377, 29), (0.032840715504398377, 28), (0.032840715504398377, 27), (0.032840715504398377, 25), (0.032840715504398377, 24), (0.032840715504398377, 23), (0.032840715504398377, 22), (0.032840715504398377, 21), (0.032840715504398377, 20), (0.032840715504398377, 19), (0.032840715504398377, 18), (0.032840715504398377, 17), (0.032840715504398377, 16), (0.032840715504398377, 15), (0.032840715504398377, 14), (0.032840715504398377, 13), (0.032840715504398377, 11), (0.032840715504398377, 10), (0.032840715504398377, 9), (0.032840715504398377, 8), (0.032840715504398377, 7), (0.032840715504398377, 6), (0.032840715504398377, 5), (0.032840715504398377, 4), (0.032840715504398377, 3), (0.032840715504398377, 2), (0.032840715504398377, 1), (0.004926178289349525, 26), (0.004926178289349525, 12), (0.004926178289349525, 0)]\n"
     ]
    }
   ],
   "source": [
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【廣編特輯】亞洲最大，以設計，創意為主題的藝術博覽會 “TOKYO DESIGN WEEK（以下稱“TDW”），10月24日至11月３日在日本東京明治神宮外苑舉辦。今年是TDW 30週年, 以“互動”為主題，彙集了設計，藝術，音樂，時尚，機器人等多種元素。現場不僅可領略多元化，新奇的創意，觀眾還可以與藝術家們互動交流。去年最具人氣的“Creative Life展（創意生活展）”，曾創下11萬人以上觀眾入場的記錄。在這區域不僅受到觀眾的歡迎，更有製造商，採購商和世界各地的設計師慕名而來。同時有許多的爆點，是各家媒體爭相報導的焦點。在“藝術設計精英展”區，來自日本國內及國外的藝術界，設計界的巨匠們以個人作品展的形式進行展示。本屆TDW前期主要展示建築設計，室內家居設計，產品設計；後期主要展示時尚，染織，攝影，影像，平面設計等。屆時，各創意設計的精英及藝術設計大師聚集於此，將成為與來自全球的採購商和超過十萬以上追求藝術生活的觀眾交流的最佳平台。受到家族客歡迎的“機器人博物館 超級機器人展“區域，以“機器人或模擬人類裝置”為題，新穎獨特的機器人表演將讓觀眾親身體驗未來設計和技術所帶來的震撼。此次TDW不僅彙集了創意生活，藝術，設計，建築，音樂，機器人等元素，更是將日本全國極具特色的創新產品，當地特產，結合高科技的產品等一併展出。觀眾可以充分體驗到原汁原味的日本魅力，享受一場充滿創意的藝術盛宴，在今年的TDW上，日本當地知名“特產”彙集一堂。而像誕生於札幌紅遍全球的“虛擬歌手”初音未來，仙台傳統藝術品木雕人偶，首次在會場發表的福岡互動裝置[森林木琴]，模擬現實沖繩石垣島自然風土的裝置系統，日本最具人氣的吉祥物熊本熊不但是當地標誌性的形象，同時他們的周邊產品也是訪日遊客首選購買的禮物。另外，大阪，神戶，倉敷都帶來了體現各自藝術品味的工藝品，這些工藝品不僅展現日本風格，還富有生活情趣。對於旅遊購物的遊客來說，參觀 TDW不僅可以感受日本的多元文化，更能一網打盡日本各地的特產。另外，參與這部分的藝術家還有在國際間非常活躍的前衛藝術家草間彌生，此次她以富士為主題，展出9件作品， 相信是不容錯過的重點之一。 更多詳情請參考官方網站：http://tokyodesignweek.jp/2015/tokyo/ \n"
     ]
    }
   ],
   "source": [
    "print content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xef in position 3: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-211-a7c15ce561d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtr4w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextRank4Keyword\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./stopword.data'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 导入停止词\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#使用词性过滤，文本小写，窗口为2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\twmtm\\textrank4zh\\TextRank4Keyword.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, stop_words_file, delimiters)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSegmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_words_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdelimiters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords_no_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m     \u001b[1;31m# 2维列表\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\twmtm\\textrank4zh\\Segmentation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, stop_words_file, delimiters)\u001b[0m\n\u001b[0;32m    102\u001b[0m         '''\n\u001b[0;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordSegmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentenceSegmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelimiters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msegment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeech_tag_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\twmtm\\textrank4zh\\Segmentation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, delimiters)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mdelimiters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m用\u001b[0m\u001b[0;31m来\u001b[0m\u001b[0;31m拆\u001b[0m\u001b[0;31m分\u001b[0m\u001b[0;31m句\u001b[0m\u001b[0;31m子\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         '''\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelimiters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelimiters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xef in position 3: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "#-*- encoding:utf-8 -*-\n",
    "import codecs\n",
    "from textrank4zh import TextRank4Keyword, TextRank4Sentence\n",
    "\n",
    "text = content\n",
    "tr4w = TextRank4Keyword(stop_words_file='./stopword.data')  # 导入停止词\n",
    "\n",
    "#使用词性过滤，文本小写，窗口为2\n",
    "tr4w.train(text=text, speech_tag_filter=True, lower=True, window=2)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xef in position 3: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-210-dab7bfa56b5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtr4w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextRank4Keyword\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./stopword.data'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 导入停止词\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#使用词性过滤，文本小写，窗口为2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\twmtm\\textrank4zh\\TextRank4Keyword.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, stop_words_file, delimiters)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSegmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_words_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdelimiters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords_no_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m     \u001b[1;31m# 2维列表\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\twmtm\\textrank4zh\\Segmentation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, stop_words_file, delimiters)\u001b[0m\n\u001b[0;32m    102\u001b[0m         '''\n\u001b[0;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordSegmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentenceSegmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelimiters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msegment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeech_tag_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\twmtm\\textrank4zh\\Segmentation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, delimiters)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mdelimiters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m用\u001b[0m\u001b[0;31m来\u001b[0m\u001b[0;31m拆\u001b[0m\u001b[0;31m分\u001b[0m\u001b[0;31m句\u001b[0m\u001b[0;31m子\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         '''\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelimiters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelimiters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xef in position 3: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "\n",
    "print '关键词：'\n",
    "# 20个关键词且每个的长度最小为1\n",
    "print '/'.join(tr4w.get_keywords(20, word_min_len=1))  \n",
    "\n",
    "print '关键短语：'\n",
    "# 20个关键词去构造短语，短语在原文本中出现次数最少为2\n",
    "print '/'.join(tr4w.get_keyphrases(keywords_num=20, min_occur_num= 2))  \n",
    "\n",
    "tr4s = TextRank4Sentence(stop_words_file='./stopword.data')\n",
    "\n",
    "# 使用词性过滤，文本小写，使用words_all_filters生成句子之间的相似性\n",
    "tr4s.train(text=text, speech_tag_filter=True, lower=True, source = 'all_filters')\n",
    "\n",
    "print '摘要：'\n",
    "print '\\n'.join(tr4s.get_key_sentences(num=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
